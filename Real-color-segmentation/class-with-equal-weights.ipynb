{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f691c8-4a18-4c73-89e8-3603df69fc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Almadina Computers\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import glob as gb\n",
    "from matplotlib import pyplot as plt\n",
    "from patchify import patchify\n",
    "from PIL import Image\n",
    "import segmentation_models as sm\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7210769-3357-4e19-90b9-301f72cb5880",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = 'Semantic dataset/'\n",
    "patch_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055eb274-0a99-486b-a6d2-bd597148b699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now patchifying image: Semantic dataset/Tile 1\\images/images0.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images1.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images10.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images100.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images103.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images104.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images105.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images106.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images107.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images108.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images109.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images11.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images110.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images111.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images113.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images115.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images116.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images118.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images119.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images12.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images120.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images122.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images123.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images124.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images127.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images128.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images129.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images13.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images130.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images131.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images132.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images133.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images134.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images135.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images136.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images137.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images138.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images139.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images14.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images140.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images141.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images142.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images143.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images144.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images145.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images146.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images147.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images148.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images149.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images15.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images150.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images151.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images152.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images153.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images154.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images155.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images156.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images157.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images158.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images159.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images16.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images160.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images161.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images162.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images163.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images165.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images167.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images168.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images169.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images17.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images172.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images174.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images176.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images177.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images179.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images18.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images181.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images182.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images184.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images185.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images186.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images188.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images189.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images19.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images190.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images191.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images192.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images193.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images194.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images195.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images197.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images198.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images199.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images2.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images20.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images200.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images201.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images203.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images206.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images207.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images208.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images209.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images21.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images210.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images211.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images213.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images214.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images215.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images216.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images217.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images218.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images219.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images22.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images220.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images221.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images222.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images223.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images225.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images227.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images228.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images229.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images23.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images231.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images232.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images233.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images234.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images235.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images236.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images237.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images238.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images24.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images240.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images241.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images242.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images243.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images244.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images246.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images247.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images248.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images25.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images251.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images253.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images255.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images258.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images260.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images262.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images263.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images264.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images265.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images266.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images267.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images268.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images269.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images27.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images271.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images272.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images273.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images275.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images276.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images277.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images279.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images28.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images281.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images282.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images283.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images284.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images285.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images286.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images287.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images288.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images289.png\n",
      "Now patchifying image: Semantic dataset/Tile 1\\images/images293.png\n"
     ]
    }
   ],
   "source": [
    "image_dataset = []  \n",
    "for path, subdirs, files in os.walk(root_directory):\n",
    "    #print(path)  \n",
    "    dirname = path.split(os.path.sep)[-1]\n",
    "    if dirname == 'images':   #Find all 'images' directories\n",
    "        images = os.listdir(path)  #List of all image names in this subdirectory\n",
    "        for i, image_name in enumerate(images):  \n",
    "            if image_name.endswith(\".png\"):   #Only read jpg images...\n",
    "               \n",
    "                image = cv2.imread(path+\"/\"+image_name, 1)  #Read each image as BGR\n",
    "                SIZE_X = (image.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "                SIZE_Y = (image.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "                image = Image.fromarray(image)\n",
    "                image = image.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n",
    "                #image = image.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n",
    "                image = np.array(image)             \n",
    "       \n",
    "                #Extract patches from each image\n",
    "                print(\"Now patchifying image:\", path+\"/\"+image_name)\n",
    "                patches_img = patchify(image, (patch_size, patch_size, 3), step=patch_size)  #Step=256 for 256 patches means no overlap\n",
    "        \n",
    "                for i in range(patches_img.shape[0]):\n",
    "                    for j in range(patches_img.shape[1]):\n",
    "                        \n",
    "                        single_patch_img = patches_img[i,j,:,:]\n",
    "                        \n",
    "                        #Use minmaxscaler instead of just dividing by 255. \n",
    "                        single_patch_img = scaler.fit_transform(single_patch_img.reshape(-1, single_patch_img.shape[-1])).reshape(single_patch_img.shape)\n",
    "                        \n",
    "                        #single_patch_img = (single_patch_img.astype('float16')) / 255. \n",
    "                        single_patch_img = single_patch_img[0] #Drop the extra unecessary dimension that patchify adds.                               \n",
    "                        image_dataset.append(single_patch_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fbbc9f-4a94-4578-b2a2-8aa05a2305a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dataset = []  \n",
    "for path, subdirs, files in os.walk(root_directory):\n",
    "    #print(path)  \n",
    "    dirname = path.split(os.path.sep)[-1]\n",
    "    if dirname == 'masks':   #Find all 'images' directories\n",
    "        masks = os.listdir(path)  #List of all image names in this subdirectory\n",
    "        for i, mask_name in enumerate(masks):  \n",
    "            if mask_name.endswith(\".png\"):   #Only read png images... (masks in this dataset)\n",
    "               \n",
    "                mask = cv2.imread(path+\"/\"+mask_name, 1)  #Read each image as Grey (or color but remember to map each color to an integer)\n",
    "                mask = cv2.cvtColor(mask,cv2.COLOR_BGR2RGB)\n",
    "                SIZE_X = (mask.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "                SIZE_Y = (mask.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "                mask = Image.fromarray(mask)\n",
    "                mask = mask.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n",
    "                #mask = mask.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n",
    "                mask = np.array(mask)             \n",
    "       \n",
    "                #Extract patches from each image\n",
    "                print(\"Now patchifying mask:\", path+\"/\"+mask_name)\n",
    "                patches_mask = patchify(mask, (patch_size, patch_size, 3), step=patch_size)  #Step=256 for 256 patches means no overlap\n",
    "        \n",
    "                for i in range(patches_mask.shape[0]):\n",
    "                    for j in range(patches_mask.shape[1]):\n",
    "                        \n",
    "                        single_patch_mask = patches_mask[i,j,:,:]\n",
    "                        #single_patch_img = (single_patch_img.astype('float32')) / 255. #No need to scale masks, but you can do it if you want\n",
    "                        single_patch_mask = single_patch_mask[0] #Drop the extra unecessary dimension that patchify adds.                               \n",
    "                        mask_dataset.append(single_patch_mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017230f-84a8-4eb4-93ec-27523569f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = np.array(image_dataset)\n",
    "mask_dataset =  np.array(mask_dataset)\n",
    "\n",
    "#Sanity check, view few mages\n",
    "import random\n",
    "import numpy as np\n",
    "image_number = random.randint(0, len(image_dataset))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(np.reshape(image_dataset[image_number], (patch_size, patch_size, 3)))\n",
    "plt.subplot(122)\n",
    "plt.imshow(np.reshape(mask_dataset[image_number], (patch_size, patch_size, 3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23335174-c160-4a27-9e6e-67f91e0b67a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "afforestation = '#32cd32'.lstrip('#')\n",
    "afforestation = np.array(tuple(int(afforestation[i:i+2], 16) for i in (0, 2, 4))) #132, 41, 246\n",
    "\n",
    "deforestation = '#ff0000'.lstrip('#') \n",
    "deforestation = np.array(tuple(int(deforestation[i:i+2], 16) for i in (0, 2, 4))) #110, 193, 228\n",
    "\n",
    "degradation =  '#ffa500'.lstrip('#') \n",
    "degradation = np.array(tuple(int(degradation[i:i+2], 16) for i in (0, 2, 4))) #254, 221, 58\n",
    "\n",
    "other_area = '#ffff00'.lstrip('#')\n",
    "other_area = np.array(tuple(int(other_area[i:i+2], 16) for i in (0, 2, 4))) # 60, 16, 152\n",
    "\n",
    "forest = '#006400'.lstrip('#') \n",
    "forest = np.array(tuple(int(forest[i:i+2], 16) for i in (0, 2, 4))) #226, 169, 41\n",
    "\n",
    "Unlabeled = '#808080'.lstrip('#')\n",
    "Unlabeled = np.array(tuple(int(Unlabeled[i:i+2], 16) for i in (0, 2, 4))) # 60, 16, 152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b5cc8-2822-455c-bf50-3e80c2751330",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = single_patch_mask\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a84f12-8410-4180-8958-10b10cdc807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_2D_label(label):\n",
    "    \n",
    "    label_seg = np.zeros(label.shape,dtype=np.uint8)\n",
    "    label_seg [np.all(label==afforestation,axis=-1)] = 0\n",
    "    label_seg [np.all(label==deforestation,axis=-1)] = 1\n",
    "    label_seg [np.all(label==degradation,axis=-1)] = 2\n",
    "    label_seg [np.all(label==forest,axis=-1)] = 3\n",
    "    label_seg [np.all(label == other_area,axis=-1)] = 4\n",
    "    label_seg [np.all(label == Unlabeled,axis=-1)] = 5\n",
    "    \n",
    "    label_seg = label_seg[:,:,0]  #Just take the first channel, no need for all 3 channels\n",
    "    \n",
    "    return label_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b221215-8896-4fc5-9192-b1d30fef1271",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in range(mask_dataset.shape[0]):\n",
    "    label = rgb_to_2D_label(mask_dataset[i])\n",
    "    labels.append(label)    \n",
    "\n",
    "labels = np.array(labels)   \n",
    "labels = np.expand_dims(labels, axis=3)\n",
    " \n",
    "print(\"Unique labels in label dataset are: \", np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b0eb1e-ba00-44d5-aea8-8f5bbe0b7aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'labels' contains the class labels for your dataset\n",
    "plt.figure(figsize=(8, 8))\n",
    "labels_name = ['afforestation', 'deforestation', 'degradation', 'forest', 'other_area', 'Unlabeled']\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "# Define colors for each class label\n",
    "color_mapping = {\n",
    "    'afforestation': '#32cd32',\n",
    "    'deforestation': '#ff0000',\n",
    "    'degradation': '#ffa500',\n",
    "    'forest': '#006400',\n",
    "    'other_area': '#ffff00',\n",
    "    'Unlabeled': '#808080'\n",
    "}\n",
    "\n",
    "colors = [color_mapping[label] for label in labels_name]\n",
    "\n",
    "sns.barplot(x=labels_name, y=counts, palette=colors)\n",
    "plt.title('ARVI: Number of Data Available for Each Class')\n",
    "plt.xlabel('Different land cover classes', fontsize=12)\n",
    "plt.ylabel('Pixel distribution for various land cover classes', fontsize=12)\n",
    "#plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e26d09-654a-4afc-8d23-d16774a07d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "image_number = random.randint(0, len(image_dataset))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(image_dataset[image_number])\n",
    "plt.subplot(122)\n",
    "plt.imshow(labels[image_number][:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f179818f-6f2a-4e09-924a-b6cd994e1ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(labels))\n",
    "print(n_classes)\n",
    "from keras.utils import to_categorical\n",
    "labels_cat = to_categorical(labels, num_classes=n_classes)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_dataset, labels_cat, test_size = 0.30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b95724-c654-4714-8f63-d0256f8b6923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the labels\n",
    "n_classes = len(np.unique(labels))\n",
    "y_train_cat = to_categorical(y_train, num_classes=n_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afbf6b8-a8b4-4745-89a8-37dffdbe97b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models as sm\n",
    "\n",
    "# Provided class weights\n",
    "weights = [1.666666, 1.666666, 1.666666, 1.666666, 1.666666, 1.666666]\n",
    "\n",
    "# Define Dice Loss with class weights\n",
    "dice_loss = sm.losses.DiceLoss(class_weights=weights)\n",
    "\n",
    "# Define Categorical Focal Loss\n",
    "focal_loss = sm.losses.CategoricalFocalLoss()\n",
    "\n",
    "# Combine the two losses with appropriate weights\n",
    "total_loss = dice_loss + (1 * focal_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0afe166-7cfd-4ff7-a9a9-04cda7872c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = X_train.shape[1]\n",
    "IMG_WIDTH  = X_train.shape[2]\n",
    "IMG_CHANNELS = X_train.shape[3]\n",
    "total_classe = y_train.shape[3]\n",
    "print(IMG_HEIGHT)\n",
    "print(IMG_WIDTH)\n",
    "print(IMG_CHANNELS)\n",
    "print(total_classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce2c58-bab2-4d6d-aec4-b0c4505725fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36af4122-a914-4dbf-833b-d2941d7eda21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacard_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eb2491-cdd0-4a45-83ee-8a01462663d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(n_classes=6, IMG_HEIGHT=224, IMG_WIDTH=224, IMG_CHANNELS=3):\n",
    "#Build the model\n",
    "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "    #s = Lambda(lambda x: x / 255)(inputs)   #No need for this if we normalize our inputs beforehand\n",
    "    s = inputs\n",
    "\n",
    "    #Contraction path\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
    "    c1 = Dropout(0.2)(c1)  # Original 0.1\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = Dropout(0.2)(c2)  # Original 0.1\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "     \n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = Dropout(0.2)(c3)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "     \n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = Dropout(0.2)(c4)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "     \n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = Dropout(0.3)(c5)\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "    \n",
    "    #Expansive path \n",
    "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
    "    c6 = Dropout(0.2)(c6)\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    "     \n",
    "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = Dropout(0.2)(c7)\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "     \n",
    "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = Dropout(0.2)(c8)  # Original 0.1\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "     \n",
    "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "    c9 = Dropout(0.2)(c9)  # Original 0.1\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "     \n",
    "    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(c9)\n",
    "     \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    #NOTE: Compile the model in the main program to make it easy to test with various loss functions\n",
    "    #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893b6577-8442-4274-a5f7-253e54626e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=['accuracy', jacard_coef]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c852fa1c-f0bc-4104-9d4a-b076d756709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    return unet_model(n_classes=n_classes, IMG_HEIGHT=IMG_HEIGHT, IMG_WIDTH=IMG_WIDTH, IMG_CHANNELS=IMG_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a8b23c-ff8a-4ff0-9c6f-3966eb4bda81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.compile(optimizer='adam', loss=total_loss, metrics=metrics)\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c7653d-6541-4877-8df4-56d78c4c6af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = model.fit(X_train, y_train_cat,\n",
    "                    batch_size = 8, \n",
    "                    verbose=1, \n",
    "                    epochs=5, \n",
    "                    validation_data=(X_test, y_test_cat), \n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b3f545-bb6d-4767-b823-d8285ea47990",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = history1.history['accuracy']\n",
    "val_accuracy = history1.history['val_accuracy']\n",
    "\n",
    "train_loss = history1.history['loss']\n",
    "val_loss = history1.history['val_loss']\n",
    "#learning_rate = history1.history['lr']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(12, 10))\n",
    "\n",
    "ax[0].set_title('Training Accuracy vs. Epochs')\n",
    "ax[0].plot(train_accuracy, 'o-', label='Train Accuracy')\n",
    "ax[0].plot(val_accuracy, 'o-', label='Validation Accuracy')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].legend(loc='best')\n",
    "\n",
    "ax[1].set_title('Training/Validation Loss vs. Epochs')\n",
    "ax[1].plot(train_loss, 'o-', label='Train Loss')\n",
    "ax[1].plot(val_loss, 'o-', label='Validation Loss')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend(loc='best')\n",
    "\n",
    "#ax[2].set_title('Learning Rate vs. Epochs')\n",
    "#ax[2].plot(learning_rate, 'o-', label='Learning Rate')\n",
    "#ax[2].set_xlabel('Epochs')\n",
    "#ax[2].set_ylabel('Loss')\n",
    "#ax[2].legend(loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#out_dir = 'C:/Users/best/Sattelite/Results'\n",
    "#conf_matrix_path = os.path.join(out_dir, 'tight_layout.png')\n",
    "#plt.imsave(conf_matrix_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af308784-a2fc-4032-ac05-a9a84eb8aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test_cat)\n",
    "print(\"Test Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac4f59a-925c-4d28-bbdd-cca5ec4ead6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_argmax = np.argmax(y_pred, axis=3)\n",
    "y_test_argmax = np.argmax(y_test_cat, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d7e6c7-48d2-4774-abfa-ed8f0c6c1d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred_argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fa2400-f3de-46c3-9701-89482a6c6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test_argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392b2db-e15d-47fc-84bc-6bb0eea3c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Flatten the predicted and ground truth labels\n",
    "y_pred_flat = y_pred_argmax.flatten()\n",
    "y_test_flat = y_test_argmax.flatten()\n",
    "\n",
    "# Define class labels\n",
    "class_labels = ['afforestation', 'deforestation', 'degradation', 'forest', 'other_area', 'Unlabeled']\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_flat, y_pred_flat)\n",
    "\n",
    "# Compute class-wise accuracy\n",
    "class_accuracy = [conf_matrix[i, i] / np.sum(conf_matrix[i, :]) for i in range(conf_matrix.shape[0])]\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print class-wise accuracy\n",
    "for i, label in enumerate(class_labels):\n",
    "    print(f'Accuracy for {label}: {class_accuracy[i]:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1cd4d1-8d3e-4432-89fd-a012019e181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Compute precision, recall, and F1 score for each class\n",
    "precision = precision_score(y_test_flat, y_pred_flat, average=None)\n",
    "recall = recall_score(y_test_flat, y_pred_flat, average=None)\n",
    "f1 = f1_score(y_test_flat, y_pred_flat, average=None)\n",
    "\n",
    "# Print precision, recall, and F1 score for each class\n",
    "for i, label in enumerate(class_labels):\n",
    "    print(f'Class: {label}')\n",
    "    print(f'  Precision: {precision[i]*100:.2f}%')\n",
    "    print(f'  Recall: {recall[i]*100:.2f}%')\n",
    "    print(f'  F1 Score: {f1[i]*100:.2f}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45317043-f6bf-49c7-8f0f-1c0b74b96b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using built in keras function for IoU\n",
    "from tensorflow.keras.metrics import MeanIoU, Accuracy\n",
    "n_classes = 6\n",
    "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras.update_state(y_test_argmax, y_pred_argmax)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c970a2c-4625-4184-9b12-e2cf2e347870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define a function to calculate IoU for each class\n",
    "def calculate_iou_per_class(conf_matrix):\n",
    "    iou_per_class = []\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        intersection = conf_matrix[i, i]\n",
    "        union = np.sum(conf_matrix[i, :]) + np.sum(conf_matrix[:, i]) - intersection\n",
    "        iou = intersection / union if union > 0 else 0\n",
    "        iou_per_class.append(iou)\n",
    "    return iou_per_class\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_flat, y_pred_flat)\n",
    "\n",
    "# Calculate IoU for each class\n",
    "iou_per_class = calculate_iou_per_class(conf_matrix)\n",
    "\n",
    "# Print IoU for each class\n",
    "for i, label in enumerate(class_labels):\n",
    "    print(f'IoU for {label}: {iou_per_class[i]*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a6711-a502-40c1-b852-b9dc4a66db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "test_img_numbers = random.sample(range(X_test.shape[0]), num_samples)\n",
    "for test_img_number in test_img_numbers:\n",
    "    test_img = X_test[test_img_number]\n",
    "    ground_truth=y_test_argmax[test_img_number]\n",
    "    #test_img_norm=test_img[:,:,0][:,:,None]\n",
    "    test_img_input=np.expand_dims(test_img, 0)\n",
    "    prediction = (model.predict(test_img_input))\n",
    "    predicted_img=np.argmax(prediction, axis=3)[0,:,:]\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(231)\n",
    "    plt.title('Testing Image')\n",
    "    plt.imshow(test_img)\n",
    "    plt.subplot(232)\n",
    "    plt.title('Testing Label')\n",
    "    plt.imshow(ground_truth, vmin=0, vmax = n_classes)\n",
    "    plt.subplot(233)\n",
    "    plt.title('Prediction on test image')\n",
    "    plt.imshow(predicted_img, vmin=0, vmax = n_classes)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b5fbd-3cfd-4be3-a9f3-0a1792bb9f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
